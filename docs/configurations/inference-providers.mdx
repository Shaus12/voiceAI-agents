---
title: "Inference Provider"
description: "VoiceFX AI ships with its own inferencing engine, which is hosted at https://services.dograh.com. The inference service provides LLM, TTS and STT services. In this document you can see how you can configure the inferencing engine to your favourite provider, like OpenAI, Gemini etc."
---

## Configure Inference Provider

You can go to `https://app.dograh.com/service-configurations` if you are on hosted version of VoiceFX AI or go to `http://localhost:3010/service-configurations` if you are running VoiceFX AI locally.

You can see the configuration for the inference provider in the following screenshot.

![Inference Provider Configuration](../images/service-configuration.png)

You can select the provider from the dropdown and configure the API key, model, etc.

## Next Steps

You can see how to configure the telephony provider in [Telephony Integrations](/telephony/twilio).
